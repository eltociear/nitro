---
title: Prompt Role Support
---

Understanding the roles of different types of prompts—system, user, and assistant—is crucial for effectively utilizing Large Language Model. These prompts work together to create a coherent and functional conversational flow.

With Nitro, developers can easily config the diaglog for "system_prompt" or implement advanced prompt engineering like [few-shot learning](https://arxiv.org/abs/2005.14165)


## System prompt
The system prompt plays a foundational role in setting up the behavior of the assistant. It can be used to guide the assistant's personality or provide specific instructions for its behavior throughout the conversation. While the system message is optional, its presence can significantly influence the assistant's responses. A generic system message, like "You are a helpful assistant," sets a baseline behavior.

## User prompt
User prompts are the requests or comments directed towards the assistant. They form the core of the conversation, with the assistant responding to these user inputs. The user messages are essential for directing the flow and nature of the conversation.

## Assistant prompt
Assistant prompts are responses or messages generated by the assistant. These can be previous responses stored in the system or examples provided by developers to demonstrate desired behavior. Assistant prompts are crucial for maintaining the context and continuity in ongoing conversations.

## Example usage

Combine all three roles we could create "Pirate" a example like this
```
curl -X POST 'http://localhost:3928/inferences/llamacpp/loadmodel' \
  -H 'Content-Type: application/json' \
  -d '{
    "llama_model_path": "/path/to/your_model.gguf",
    "ctx_len": 128,
    "ngl": 100,
    "pre_prompt": "You are a Pirate. Using drunk language with a lot of Arr...",
    "user_prompt": "USER: ",
    "ai_prompt": "ASSISTANT: "
  }'
```

(We should have a comparation between llama.cpp doesn't have a system prompt so can't output as good as system prompt implemented via Nitro)
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"category","label":"Introduction","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"About Nitro","href":"/about","docId":"new/about"},{"type":"link","label":"Quickstart","href":"/quickstart","docId":"new/quickstart"},{"type":"link","label":"Installation","href":"/install","docId":"new/install"},{"type":"link","label":"Build From Source","href":"/build-source","docId":"new/build-source"}]},{"type":"category","label":"Features","items":[{"type":"link","label":"Chat Completion","href":"/features/chat","docId":"features/chat"},{"type":"link","label":"Embedding","href":"/features/embed","docId":"features/embed"},{"type":"link","label":"Multithreading","href":"/features/multi-thread","docId":"features/multi-thread"},{"type":"link","label":"Continuous Batching","href":"/features/cont-batch","docId":"features/cont-batch"},{"type":"link","label":"Load and Unload models","href":"/features/load-unload","docId":"features/load-unload"},{"type":"link","label":"Warming Up Model","href":"/features/warmup","docId":"features/warmup"},{"type":"link","label":"Prompt Role Support","href":"/features/prompt","docId":"features/prompt"}],"collapsed":true,"collapsible":true,"href":"/features/feat"},{"type":"category","label":"Guides","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Nitro with Chatbox","href":"/examples/chatbox","docId":"examples/chatbox"},{"type":"link","label":"Nitro with openai-node","href":"/examples/openai-node","docId":"examples/openai-node"},{"type":"link","label":"Nitro with openai-python","href":"/examples/openai-python","docId":"examples/openai-python"}]},{"type":"link","label":"FAQs","href":"/faq","docId":"new/faq"}],"apiSidebar":[{"type":"link","label":"api-reference","href":"/api-reference","docId":"api-reference"}]},"docs":{"api-reference":{"id":"api-reference","title":"api-reference","description":"","sidebar":"apiSidebar"},"examples/chatbox":{"id":"examples/chatbox","title":"Nitro with Chatbox","description":"This guide demonstrates how to integrate Nitro with Chatbox, showcasing the compatibility of Nitro with various platforms.","sidebar":"docsSidebar"},"examples/llm":{"id":"examples/llm","title":"Simple chatbot with Nitro","description":"This guide provides instructions to create a chatbot powered by Nitro using the GGUF model."},"examples/openai-node":{"id":"examples/openai-node","title":"Nitro with openai-node","description":"You can migrate from OAI API or Azure OpenAI to Nitro using your existing NodeJS code quickly","sidebar":"docsSidebar"},"examples/openai-python":{"id":"examples/openai-python","title":"Nitro with openai-python","description":"You can migrate from OAI API or Azure OpenAI to Nitro using your existing Python code quickly","sidebar":"docsSidebar"},"features/chat":{"id":"features/chat","title":"Chat Completion","description":"The Chat Completion feature in Nitro provides a flexible way to interact with any local Large Language Model (LLM).","sidebar":"docsSidebar"},"features/cont-batch":{"id":"features/cont-batch","title":"Continuous Batching","description":"What is continous batching?","sidebar":"docsSidebar"},"features/embed":{"id":"features/embed","title":"Embedding","description":"What are embeddings?","sidebar":"docsSidebar"},"features/feat":{"id":"features/feat","title":"Nitro Features","description":"Nitro enhances the llama.cpp research base, optimizing it for production environments with advanced features:","sidebar":"docsSidebar"},"features/load-unload":{"id":"features/load-unload","title":"Load and Unload models","description":"Load model","sidebar":"docsSidebar"},"features/multi-thread":{"id":"features/multi-thread","title":"Multithreading","description":"What is Multithreading?","sidebar":"docsSidebar"},"features/prompt":{"id":"features/prompt","title":"Prompt Role Support","description":"System, user, and assistant prompt is crucial for effectively utilizing the Large Language Model. These prompts work together to create a coherent and functional conversational flow.","sidebar":"docsSidebar"},"features/warmup":{"id":"features/warmup","title":"Warming Up Model","description":"What is Model Warming Up?","sidebar":"docsSidebar"},"new/about":{"id":"new/about","title":"About Nitro","description":"Nitro is a high-efficiency C++ inference engine for edge computing, powering Jan. It is lightweight and embeddable, ideal for product integration.","sidebar":"docsSidebar"},"new/architecture":{"id":"new/architecture","title":"Architecture","description":"Nitro Architecture"},"new/build-source":{"id":"new/build-source","title":"Build From Source","description":"This guide provides step-by-step instructions for building Nitro from source on Linux, macOS, and Windows systems.","sidebar":"docsSidebar"},"new/faq":{"id":"new/faq","title":"FAQs","description":"1. Is Nitro the same as Llama.cpp with an API server?","sidebar":"docsSidebar"},"new/install":{"id":"new/install","title":"Installation","description":"This guide provides instructions for installing Nitro using the provided install.sh and install.bat scripts for Linux, macOS, and Windows systems.","sidebar":"docsSidebar"},"new/model-cycle":{"id":"new/model-cycle","title":"Model Life Cycle","description":"Load model"},"new/quickstart":{"id":"new/quickstart","title":"Quickstart","description":"Step 1: Install Nitro","sidebar":"docsSidebar"}}}')}}]);
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[372],{3905:(t,e,n)=>{n.d(e,{Zo:()=>u,kt:()=>g});var r=n(7294);function a(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function l(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);e&&(r=r.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,r)}return n}function o(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?l(Object(n),!0).forEach((function(e){a(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function i(t,e){if(null==t)return{};var n,r,a=function(t,e){if(null==t)return{};var n,r,a={},l=Object.keys(t);for(r=0;r<l.length;r++)n=l[r],e.indexOf(n)>=0||(a[n]=t[n]);return a}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(r=0;r<l.length;r++)n=l[r],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(a[n]=t[n])}return a}var p=r.createContext({}),d=function(t){var e=r.useContext(p),n=e;return t&&(n="function"==typeof t?t(e):o(o({},e),t)),n},u=function(t){var e=d(t.components);return r.createElement(p.Provider,{value:e},t.children)},s="mdxType",m={inlineCode:"code",wrapper:function(t){var e=t.children;return r.createElement(r.Fragment,{},e)}},c=r.forwardRef((function(t,e){var n=t.components,a=t.mdxType,l=t.originalType,p=t.parentName,u=i(t,["components","mdxType","originalType","parentName"]),s=d(n),c=a,g=s["".concat(p,".").concat(c)]||s[c]||m[c]||l;return n?r.createElement(g,o(o({ref:e},u),{},{components:n})):r.createElement(g,o({ref:e},u))}));function g(t,e){var n=arguments,a=e&&e.mdxType;if("string"==typeof t||a){var l=n.length,o=new Array(l);o[0]=c;var i={};for(var p in e)hasOwnProperty.call(e,p)&&(i[p]=e[p]);i.originalType=t,i[s]="string"==typeof t?t:a,o[1]=i;for(var d=2;d<l;d++)o[d]=n[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},6506:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>p,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>d});var r=n(7462),a=(n(7294),n(3905));const l={title:"Quick start"},o=void 0,i={unversionedId:"nitro/using-nitro",id:"nitro/using-nitro",title:"Quick start",description:"Step 1: Download Nitro",source:"@site/docs/nitro/using-nitro.md",sourceDirName:"nitro",slug:"/nitro/using-nitro",permalink:"/nitro/using-nitro",draft:!1,editUrl:"https://github.com/janhq/nitro/tree/main/docs/docs/nitro/using-nitro.md",tags:[],version:"current",lastUpdatedBy:"Hoang Ha",lastUpdatedAt:1699872367,formattedLastUpdatedAt:"Nov 13, 2023",frontMatter:{title:"Quick start"},sidebar:"docsSidebar",previous:{title:"Installation",permalink:"/nitro/installation"},next:{title:"Troubleshooting Nitro",permalink:"/guides/troubleshooting"}},p={},d=[{value:"Step 1: Download Nitro",id:"step-1-download-nitro",level:2},{value:"Step 2: Download a Model",id:"step-2-download-a-model",level:2},{value:"Step 3: Run Nitro",id:"step-3-run-nitro",level:2},{value:"Step 4: Nitro Inference",id:"step-4-nitro-inference",level:2}],u={toc:d},s="wrapper";function m(t){let{components:e,...n}=t;return(0,a.kt)(s,(0,r.Z)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"step-1-download-nitro"},"Step 1: Download Nitro"),(0,a.kt)("p",null,"To use Nitro, download the released binaries from the release page below:"),(0,a.kt)("p",null,"\ud83d\udd17 ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/janhq/nitro/releases"},"Download Nitro")),(0,a.kt)("p",null,"After downloading the release, double-click on the Nitro binary."),(0,a.kt)("h2",{id:"step-2-download-a-model"},"Step 2: Download a Model"),(0,a.kt)("p",null,'Download a llama model to try running the llama C++ integration. You can find a "GGUF" model on The Bloke\'s page below:'),(0,a.kt)("p",null,"\ud83d\udd17 ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/TheBloke"},"Download Model")),(0,a.kt)("h2",{id:"step-3-run-nitro"},"Step 3: Run Nitro"),(0,a.kt)("p",null,"Double-click on Nitro to run it. After downloading your model, make sure it's saved to a specific path. Then, make an API call to load your model into Nitro."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-zsh"},'curl -X POST \'http://localhost:3928/inferences/llamacpp/loadmodel\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n    "llama_model_path": "/path/to/your_model.gguf",\n    "ctx_len": 2048,\n    "ngl": 100,\n    "embedding": true,\n    "n_parallel": 4,\n    "pre_prompt": "A chat between a curious user and an artificial intelligence",\n    "user_prompt": "USER: ",\n    "ai_prompt": "ASSISTANT: "\n  }\'\n')),(0,a.kt)("p",null,"Configure your system by setting the following parameters in your ",(0,a.kt)("inlineCode",{parentName:"p"},"jsonBody"),". The table below outlines each parameter and its description:"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,a.kt)("th",{parentName:"tr",align:null},"Type"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"llama_model_path")),(0,a.kt)("td",{parentName:"tr",align:null},"String"),(0,a.kt)("td",{parentName:"tr",align:null},"The file path to the LLaMA model.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"ngl")),(0,a.kt)("td",{parentName:"tr",align:null},"Integer"),(0,a.kt)("td",{parentName:"tr",align:null},"The number of GPU layers to use.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"ctx_len")),(0,a.kt)("td",{parentName:"tr",align:null},"Integer"),(0,a.kt)("td",{parentName:"tr",align:null},"The context length for the model operations.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"embedding")),(0,a.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,a.kt)("td",{parentName:"tr",align:null},"Whether to use embedding in the model.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"n_parallel")),(0,a.kt)("td",{parentName:"tr",align:null},"Integer"),(0,a.kt)("td",{parentName:"tr",align:null},"The number of parallel operations. Uses Drogon thread count if not set.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"cont_batching")),(0,a.kt)("td",{parentName:"tr",align:null},"Boolean"),(0,a.kt)("td",{parentName:"tr",align:null},"Whether to use continuous batching.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"user_prompt")),(0,a.kt)("td",{parentName:"tr",align:null},"String"),(0,a.kt)("td",{parentName:"tr",align:null},"The prompt to use for the user.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"ai_prompt")),(0,a.kt)("td",{parentName:"tr",align:null},"String"),(0,a.kt)("td",{parentName:"tr",align:null},"The prompt to use for the AI assistant.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"system_prompt")),(0,a.kt)("td",{parentName:"tr",align:null},"String"),(0,a.kt)("td",{parentName:"tr",align:null},"The prompt to use for system rules.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"pre_prompt")),(0,a.kt)("td",{parentName:"tr",align:null},"String"),(0,a.kt)("td",{parentName:"tr",align:null},"The prompt to use for internal configuration.")))),(0,a.kt)("h2",{id:"step-4-nitro-inference"},"Step 4: Nitro Inference"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-zsh"},'curl --location \'http://localhost:3928/inferences/llamacpp/chat_completion\' \\\n     --header \'Content-Type: application/json\' \\\n     --header \'Accept: text/event-stream\' \\\n     --header \'Access-Control-Allow-Origin: *\' \\\n     --data \'{\n        "messages": [\n            {"content": "Hello there \ud83d\udc4b", "role": "assistant"},\n            {"content": "Can you write a long story", "role": "user"}\n        ],\n        "stream": true,\n        "model": "gpt-3.5-turbo",\n        "max_tokens": 2000\n     }\'\n')),(0,a.kt)("p",null,"Nitro server is compatible with the OpenAI format, so you can expect the same output as the OpenAI ChatGPT API."))}m.isMDXComponent=!0}}]);
"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4450],{81020:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>i});var d=n(85893),l=n(11151);const o={title:"Load and Unload models"},s=void 0,r={id:"features/load-unload",title:"Load and Unload models",description:"Load model",source:"@site/docs/features/load-unload.md",sourceDirName:"features",slug:"/features/load-unload",permalink:"/features/load-unload",draft:!1,unlisted:!1,editUrl:"https://github.com/janhq/nitro/tree/main/docs/docs/features/load-unload.md",tags:[],version:"current",lastUpdatedBy:"automaticcat",lastUpdatedAt:1700820699,formattedLastUpdatedAt:"Nov 24, 2023",frontMatter:{title:"Load and Unload models"},sidebar:"docsSidebar",previous:{title:"Continuous Batching",permalink:"/features/cont-batch"},next:{title:"Warming Up Model",permalink:"/features/warmup"}},a={},i=[{value:"Load model",id:"load-model",level:2},{value:"Enabling GPU Inference",id:"enabling-gpu-inference",level:3},{value:"Unload model",id:"unload-model",level:2},{value:"Status",id:"status",level:2},{value:"Table of parameters",id:"table-of-parameters",level:3}];function c(e){const t={code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,l.a)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(t.h2,{id:"load-model",children:"Load model"}),"\n",(0,d.jsxs)(t.p,{children:["The ",(0,d.jsx)(t.code,{children:"loadmodel"})," in Nitro lets you load a local model into the server. It's an upgrade from ",(0,d.jsx)(t.code,{children:"llama.cpp"}),", offering more features and customization options."]}),"\n",(0,d.jsx)(t.p,{children:"You can load the model using:"}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-bash",metastring:'title="Load Model" {1}',children:'curl http://localhost:3928/inferences/llamacpp/loadmodel \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n    "llama_model_path": "/path/to/your_model.gguf",\n    "ctx_len": 512,\n  }\'\n'})}),"\n",(0,d.jsx)(t.p,{children:"For more detail on the loading model, please refer to [Table of parameters].(#table-of-parameters)."}),"\n",(0,d.jsx)(t.h3,{id:"enabling-gpu-inference",children:"Enabling GPU Inference"}),"\n",(0,d.jsx)(t.p,{children:"To enable GPU inference in Nitro, a simple POST request is used. This request will instruct Nitro to load the specified model into the GPU, significantly boosting the inference throughput."}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-bash",metastring:'title="GPU enable" {5}',children:'curl http://localhost:3928/inferences/llamacpp/loadmodel \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n    "llama_model_path": "/path/to/your_model.gguf",\n    "ctx_len": 512,\n    "ngl": 100,\n  }\'\n'})}),"\n",(0,d.jsxs)(t.p,{children:["You can adjust the ",(0,d.jsx)(t.code,{children:"ngl"})," parameter based on your requirements and GPU capabilities."]}),"\n",(0,d.jsx)(t.h2,{id:"unload-model",children:"Unload model"}),"\n",(0,d.jsxs)(t.p,{children:["To unload a model, you can use a similar ",(0,d.jsx)(t.code,{children:"curl"})," command as loading the model, adjusting the endpoint to ",(0,d.jsx)(t.code,{children:"/unloadmodel."})]}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-bash",metastring:'title="Unload the model" {1}',children:"curl http://localhost:3928/inferences/llamacpp/unloadmodel\n"})}),"\n",(0,d.jsx)(t.h2,{id:"status",children:"Status"}),"\n",(0,d.jsxs)(t.p,{children:["The ",(0,d.jsx)(t.code,{children:"modelStatus"})," function provides the current status of the model, including whether it is loaded and its properties. This function offers improved monitoring capabilities compared to ",(0,d.jsx)(t.code,{children:"llama.cpp"}),"."]}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-bash",metastring:'title="Check Model Status" {1}',children:"curl http://localhost:3928/inferences/llamacpp/modelstatus\n"})}),"\n",(0,d.jsx)(t.p,{children:"If you load the model correctly, the response would be"}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-js",metastring:'title="Load Model Sucessfully"',children:'{"message":"Model loaded successfully", "code": "ModelloadedSuccessfully"}\n'})}),"\n",(0,d.jsx)(t.p,{children:"In case you got error while loading models. Please check for the correct model path."}),"\n",(0,d.jsx)(t.pre,{children:(0,d.jsx)(t.code,{className:"language-js",metastring:'title="Load Model Failed"',children:'{"message":"No model loaded", "code": "NoModelLoaded"}\n'})}),"\n",(0,d.jsx)(t.h3,{id:"table-of-parameters",children:"Table of parameters"}),"\n",(0,d.jsxs)(t.table,{children:[(0,d.jsx)(t.thead,{children:(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.th,{children:"Parameter"}),(0,d.jsx)(t.th,{children:"Type"}),(0,d.jsx)(t.th,{children:"Description"})]})}),(0,d.jsxs)(t.tbody,{children:[(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"llama_model_path"})}),(0,d.jsx)(t.td,{children:"String"}),(0,d.jsx)(t.td,{children:"The file path to the LLaMA model."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"ngl"})}),(0,d.jsx)(t.td,{children:"Integer"}),(0,d.jsx)(t.td,{children:"The number of GPU layers to use."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"ctx_len"})}),(0,d.jsx)(t.td,{children:"Integer"}),(0,d.jsx)(t.td,{children:"The context length for the model operations."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"embedding"})}),(0,d.jsx)(t.td,{children:"Boolean"}),(0,d.jsx)(t.td,{children:"Whether to use embedding in the model."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"n_parallel"})}),(0,d.jsx)(t.td,{children:"Integer"}),(0,d.jsx)(t.td,{children:"The number of parallel operations. Uses Drogon thread count if not set."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"cont_batching"})}),(0,d.jsx)(t.td,{children:"Boolean"}),(0,d.jsx)(t.td,{children:"Whether to use continuous batching."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"user_prompt"})}),(0,d.jsx)(t.td,{children:"String"}),(0,d.jsx)(t.td,{children:"The prompt to use for the user."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"ai_prompt"})}),(0,d.jsx)(t.td,{children:"String"}),(0,d.jsx)(t.td,{children:"The prompt to use for the AI assistant."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"system_prompt"})}),(0,d.jsx)(t.td,{children:"String"}),(0,d.jsx)(t.td,{children:"The prompt for system rules."})]}),(0,d.jsxs)(t.tr,{children:[(0,d.jsx)(t.td,{children:(0,d.jsx)(t.code,{children:"pre_prompt"})}),(0,d.jsx)(t.td,{children:"String"}),(0,d.jsx)(t.td,{children:"The prompt to use for internal configuration."})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,l.a)(),...e.components};return t?(0,d.jsx)(t,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}},11151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>s});var d=n(67294);const l={},o=d.createContext(l);function s(e){const t=d.useContext(o);return d.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:s(e.components),d.createElement(o.Provider,{value:t},e.children)}}}]);
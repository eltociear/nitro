"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2579],{41837:e=>{e.exports=JSON.parse('{"url":"redocusaurus/plugin-redoc-0.yaml","themeId":"theme-redoc","isSpecFile":true,"spec":{"openapi":"3.0.0","info":{"title":"Nitro API","description":"Please see https://nitro.jan.ai/ for documentation."},"version":"0.1.19","contact":{"name":"Nitro Discord","url":"https://github.com/janhq/nitro"},"license":{"name":"AGPLv3","url":"https://github.com/janhq/nitro/blob/main/LICENSE"},"servers":[{"url":"https://localhost:3928/"}],"tags":[{"name":"Chat Completion","description":"Given a list of messages comprising a conversation, the model will return a response."},{"name":"Embeddings","description":"Get a vector representation of a given input."},{"name":"Health Check","description":"Check current status of the Nitro server."},{"name":"Load Model","description":"Load model to Nitro Inference Server."},{"name":"Unload Model","description":"Unload model out of Nitro Inference Server."},{"name":"Status","description":"Check current status of the model."}],"x-tagGroups":[{"name":"OpenAI Compatible","tags":["Chat Completion","Embeddings"]},{"name":"Nitro Operations","tags":["Health Check","Load Model","Unload Model","Status"]}],"paths":{"/healthz":{"get":{"operationId":"heathCheck","tags":["Health Check"],"summary":"Check the status of Nitro Server.","x-codeSamples":[{"lang":"curl","source":"curl http://localhost:3928/healthz\\n"}],"responses":{"200":{"description":"Nitro health check","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HealthcheckResponse"}}}}}}},"/inferences/llamacpp/loadmodel":{"post":{"operationId":"loadmodel","tags":["Load Model"],"summary":"Load model to Nitro Inference Server.","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/LoadModelRequest"}}}},"x-codeSamples":[{"lang":"curl","source":"curl http://localhost:3928/inferences/llamacpp/loadmodel \\\\\\n      -H \'Content-Type: application/json\' \\\\\\n      -d \'{\\n        \\"llama_model_path\\": \\"/path/to/your_model.gguf\\",\\n        \\"ctx_len\\": 512,\\n      }\'\\n"}],"responses":{"200":{"description":"Model loaded","content":{"application/json":{"schema":{"$ref":"#/components/schemas/LoadModelResponse"}}}}}}},"/inferences/llamacpp/unloadmodel":{"get":{"operationId":"unloadmodel","tags":["Unload Model"],"summary":"Unload model from Nitro Inference Server.","x-codeSamples":[{"lang":"curl","source":"curl http://localhost:3928/inferences/llamacpp/unloadmodel\\n"}],"responses":{"200":{"description":"Model unloaded","content":{"application/json":{"schema":{"$ref":"#/components/schemas/UnloadModelResponse"}}}}}}},"/inferences/llamacpp/modelstatus":{"get":{"operationId":"modelstatus","tags":["Status"],"summary":"Check status of the model on Nitro server","content":{"application/json":{"schema":{"$ref":"#/components/schemas/StatusRequest"}}},"x-codeSamples":[{"lang":"curl","source":"curl http://localhost:3928/inferences/llamacpp/modelstatus\\n"}],"responses":{"200":{"description":"Check status","content":{"application/json":{"schema":{"$ref":"#/components/schemas/StatusResponse"}}}}}}},"/v1/embeddings":{"post":{"operationId":"createEmbedding","tags":["Embeddings"],"summary":"Creates an embedding vector representing the input text.","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/CreateEmbeddingRequest"}}}},"x-codeSamples":[{"lang":"curl","source":"curl http://localhost:3928/inferences/llamacpp/embedding \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n        \\"input\\": \\"hello\\",\\n        \\"encoding_format\\": \\"float\\"\\n    }\'\\n"}],"responses":{"200":{"description":"OK","content":{"application/json":{"schema":{"$ref":"#/components/schemas/CreateEmbeddingResponse"}}}}}}},"/v1/chat/completions":{"post":{"operationId":"createChatCompletion","tags":["Chat Completion"],"summary":"Create a chat with the model.","requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/ChatCompletionRequest"}}}},"x-codeSamples":[{"lang":"curl","source":"curl -X POST \'http://localhost:3928/inferences/llamacpp/chat_completion\' \\\\\\n      -H \\"Content-Type: application/json\\" \\\\\\n      -d \'{\\n        \\"llama_model_path\\": \\"/path/to/your/model.gguf\\",\\n        \\"messages\\": [\\n          {\\n            \\"role\\": \\"user\\",\\n            \\"content\\": \\"hello\\"\\n          },\\n        ]\\n      }\'\\n"}],"responses":{"200":{"description":"OK","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ChatCompletionResponse"}}}}}}}},"components":{"schemas":{"LoadModelRequest":{"type":"object","properties":{"llama_model_path":{"type":"string","required":true,"description":"Path to your local LLM","example":"nitro/model/zephyr-7b-beta.Q5_K_M.gguf"},"ngl":{"type":"number","default":100,"minimum":0,"maximum":100,"nullable":true,"description":"The number of layers to load onto the GPU for acceleration."},"ctx_len":{"type":"number","default":2048,"nullable":true,"description":"The context length for model operations varies; the maximum depends on the specific model used."},"embedding":{"default":true,"type":"boolean","nullable":true,"description":"Whether to enable embedding."},"cont_batching":{"type":"boolean","default":false,"nullable":true,"description":"Whether to use continuous batching."},"n_parallel":{"type":"integer","default":1,"example":1,"nullable":true,"description":"The number of parallel operations. Only set when enable continuous batching."},"cpu_threads":{"type":"integer","example":4,"nullable":true,"description":"The number of threads for CPU-based inference."},"pre_prompt":{"type":"string","default":"A chat between a curious user and an artificial intelligence assistant. The assistant follows the given rules no matter what.","nullable":true,"description":"The prompt to use for internal configuration."},"system_prompt":{"type":"string","default":"ASSISTANT\'s RULE:","nullable":true,"description":"The prefix for system prompt"},"user_prompt":{"type":"string","default":"USER:","nullable":true,"description":"The prefix for user prompt."},"ai_prompt":{"type":"string","default":"ASSISTANT:","nullable":true,"description":"The prefix for assistant prompt."},"clean_cache_threshold":{"type":"integer","default":5,"nullable":true,"description":"Number of chats that will trigger clean cache action."}},"required":["llama_model_path"]},"LoadModelResponse":{"type":"object","properties":{"message":{"example":"Model loaded successfully","description":"A status indicator for when the model is successfully loaded.","anyOf":[{"type":"string","title":"Success","description":"The output will be \\"Model loaded successfully\\""},{"type":"string","title":"Failed","description":"The output will be \\"No model loaded\\""}]},"code":{"example":"Model loaded successfully","description":"A response code for Localization Support.","anyOf":[{"type":"string","title":"Success","description":"The output will be \\"Model loaded successfully\\""},{"type":"string","title":"Failed","description":"The output will be \\"No model loaded\\""}]}}},"HealthcheckRequest":{"type":"object"},"HealthcheckResponse":{"type":"object","properties":{"message":{"example":"Nitro is alive!!!","description":"A status indicator for when the model is successfully loaded.","anyOf":[{"type":"string","title":"Success","description":"The output will be \\"Nitro is alive!!!\\""},{"type":"string","title":"Failed","description":"curl: (7) Failed to connect to localhost port 3928 after 0 ms: Connection refused"}]}}},"UnloadModelRequest":{"type":"object","properties":{"message":{"example":"TODO","description":"TODO"}}},"UnloadModelResponse":{"type":"object","properties":{"message":{"example":"Model unloaded successfully","description":"A status for successful model unloading.","anyOf":[{"type":"string","title":"Success","description":"The output will be \\"Model unloaded successfully\\""},{"type":"string","title":"Failed","description":"The output will be \\"No model loaded\\""}]}}},"StatusRequest":{"type":"object","properties":{"message":{"example":"Model unloaded successfully","description":"A status for successful model unloading."}}},"StatusResponse":{"type":"object","description":"State of the loaded model","properties":{"model_data":{"type":"object","description":"Configuration data of the model","properties":{"model_loaded":{"type":"boolean","example":true,"nullable":true,"description":"A status for loading model to Nitro server."},"frequency_penalty":{"type":"number","description":"Adjusts likelihood of repeating words in the output, with a higher value discouraging repetition.","default":0,"nullable":true,"max":2,"min":0},"grammar":{"type":"string","default":"","nullable":true,"description":"Specifies grammar constraints to be applied, with an empty string implying no constraints."},"ignore_eos":{"type":"boolean","default":false,"nullable":true,"description":"Determines if the model should consider end-of-sequence tokens, with false indicating they are considered."},"logit_bias":{"type":"arrays","default":[],"description":"An array for applying biases to certain tokens\' logits to affect their selection probability."},"mirostat":{"type":"number","default":0,"nullable":true,"description":"Enables or disables the Mirostat algorithm for controlling output diversity."},"mirostat_eta":{"type":"number","default":0.1,"nullable":true,"description":"Parameter related to output diversity."},"mirostat_tau":{"type":"number","default":5,"nullable":true,"description":"Controls the temperature for the mirostat."},"model":{"type":"string","example":"nitro/model/zephyr-7b-beta.Q5_K_M.gguf","nullable":true,"description":"This is automatically set to the model you\'ve loaded on the Nitro server."},"n_ctx":{"type":"number","default":42,"nullable":true,"description":"Number of tokens in the model\'s context window."},"n_keep":{"type":"number","default":0,"nullable":true,"description":"Number of tokens to keep from the beginning of the input."},"n_predict":{"type":"number","default":100,"nullable":true,"description":"Number of tokens the model should predict, with -1 indicating no specific limit."},"n_probs":{"type":"number","default":0,"nullable":true,"description":"Controls the number of probabilities returned by the model."},"penalize_nl":{"type":"boolean","default":true,"nullable":true,"description":"Penalizes new lines in the output to make them less likely."},"presence_penalty":{"type":"number","default":0,"nullable":true,"description":"Adjusts likelihood of introducing new concepts in the output."},"repeat_last_n":{"type":"number","default":64,"nullable":true,"description":"Number of tokens to check for repetition."},"repeat_penalty":{"type":"number","default":1.1,"nullable":true,"description":"Penalizes repetitions of phrases in the last `repeat_last_n` tokens."},"seed":{"type":"number","default":4294967295,"nullable":true,"description":"Random seed for ensuring reproducibility."},"stop":{"type":"arrays","default":["hello","USER: "],"nullable":true,"description":"A list of tokens that signal the model to stop generating further output."},"stream":{"type":"boolean","default":true,"nullable":true,"description":"Determines if output generation is in a streaming manner."},"temp":{"type":"number","default":0.7,"min":0,"max":1,"nullable":true,"description":"Controls randomness of the output."},"tfs_z":{"type":"number","default":1,"nullable":true,"description":"A parameter likely related to internal model processing."},"top_k":{"type":"number","default":40,"nullable":true,"description":"Limits the number of highest probability tokens considered at each generation step."},"top_p":{"type":"number","default":0.95,"min":0,"max":1,"nullable":true,"description":"Chooses from the top tokens cumulatively making up a specified probability."},"typical_p":{"type":"number","default":1,"nullable":true,"description":"Controls output diversity, typically used alongside `top_p`."}}}}},"CreateEmbeddingRequest":{"type":"object","additionalProperties":false,"properties":{"input":{"description":"Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays.","example":"hello"},"encoding_format":{"description":"Encoding format","example":"float"}}},"CreateEmbeddingResponse":{"type":"object","description":"Response containing embeddings and related information","properties":{"data":{"type":"array","description":"Array of embedding objects","items":{"type":"object","properties":{"embedding":{"type":"array","description":"Array representing the embedding vector","items":{"type":"arrays","example":[0.06781931221485138,0.17273959517478943,-0.31053683161735535,"...",0.361769437789917]}},"index":{"type":"integer","description":"Index of the embedding in the array","example":0},"object":{"type":"string","description":"Type of the object","example":"embedding"}}}},"model":{"type":"string","description":"Model identifier","example":"_"},"object":{"type":"string","description":"Type of the overall response object","example":"list"},"usage":{"type":"object","description":"Information about token usage in the request","properties":{"prompt_tokens":{"type":"integer","description":"Number of tokens used in the prompt","example":33},"total_tokens":{"type":"integer","description":"Total number of tokens involved in the operation","example":533}}}}},"ChatCompletionRequest":{"type":"object","properties":{"messages":{"type":"arrays","description":"Contains input data or prompts for the model to process","example":[{"content":"Hello there :wave:","role":"assistant"},{"content":"Can you write a long story","role":"user"}]},"stream":{"type":"boolean","default":true,"description":"Enables continuous output generation, allowing for streaming of model responses"},"model":{"type":"string","example":"gpt-3.5-turbo","description":"Specifies the model being used for inference or processing tasks"},"max_tokens":{"type":"number","default":2048,"description":"The maximum number of tokens the model will generate in a single response"},"stop":{"type":"arrays","example":["hello"],"description":"Defines specific tokens or phrases at which the model will stop generating further output"},"frequency_penalty":{"type":"number","min":0,"max":2,"default":0,"description":"Adjusts the likelihood of the model repeating words or phrases in its output"},"presence_penalty":{"type":"number","default":0,"min":0,"max":2,"description":"Influences the generation of new and varied concepts in the model\'s output"},"temperature":{"type":"number","default":0.7,"min":0,"max":1,"description":"Controls the randomness of the model\'s output"},"top_p":{"type":"number","default":0.95,"min":0,"max":1,"description":"Set probability threshold for more relevant outputs"}}},"ChatCompletionResponse":{"type":"object","description":"Description of the response structure","properties":{"choices":{"type":"array","description":"Array of choice objects","items":{"type":"object","properties":{"finish_reason":{"type":"string","nullable":true,"example":null,"description":"Reason for finishing the response, if applicable"},"index":{"type":"integer","example":0,"description":"Index of the choice"},"message":{"type":"object","properties":{"content":{"type":"string","example":"Hello user. What can I help you with?","description":"Content of the message"},"role":{"type":"string","example":"assistant","description":"Role of the sender"}}}}}},"created":{"type":"integer","example":1700193928,"description":"Timestamp of when the response was created"},"id":{"type":"string","example":"ebwd2niJvJB1Q2Whyvkz","description":"Unique identifier of the response"},"model":{"type":"string","nullable":true,"example":"_","description":"Model used for generating the response"},"object":{"type":"string","example":"chat.completion","description":"Type of the response object"},"system_fingerprint":{"type":"string","nullable":true,"example":"_","description":"System fingerprint"},"usage":{"type":"object","description":"Information about the usage of tokens","properties":{"completion_tokens":{"type":"integer","example":500,"description":"Number of tokens used for completion"},"prompt_tokens":{"type":"integer","example":33,"description":"Number of tokens used in the prompt"},"total_tokens":{"type":"integer","example":533,"description":"Total number of tokens used"}}}}}}}}}')}}]);